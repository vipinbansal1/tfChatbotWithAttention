{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mf606MfgXeB7"
   },
   "source": [
    "# Building a Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base reference https://tutorials.botsfloor.com/how-to-build-your-first-chatbot-c84495d4622d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "lHczOO4wXeCF",
    "outputId": "8994d8ae-1e6d-4b63-c49f-7908589f09b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import time\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zie64D3ZXeCo"
   },
   "source": [
    "### Inspect and Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t0WWWgQgXeCq"
   },
   "outputs": [],
   "source": [
    "# Load movie conversation data\n",
    "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2WyBMQmgXeDU"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map each line's id with its text\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkNGvagnXeDd"
   },
   "outputs": [],
   "source": [
    "# Create a list of all of the conversations' lines' ids.\n",
    "convs = [ ]\n",
    "for line in conv_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convs.append(_line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5S-94oGUXeEC"
   },
   "outputs": [],
   "source": [
    "# Sort the sentences into questions (inputs) and answers (targets)\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for conv in convs:\n",
    "    for i in range(len(conv)-1):\n",
    "        questions.append(id2line[conv[i]])\n",
    "        answers.append(id2line[conv[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0Lp0SgxXeEx"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RpGzWOmRXeE_"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_questions = []\n",
    "for question in questions:\n",
    "    clean_questions.append(clean_text(question))\n",
    "    \n",
    "clean_answers = []    \n",
    "for answer in answers:\n",
    "    clean_answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kxEBYSUWXeFb"
   },
   "outputs": [],
   "source": [
    "# Find the length of sentences\n",
    "lengths = []\n",
    "for question in clean_questions:\n",
    "    lengths.append(len(question.split()))\n",
    "for answer in clean_answers:\n",
    "    lengths.append(len(answer.split()))\n",
    "\n",
    "# Create a dataframe so that the values can be inspected\n",
    "lengths = pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_tpRKs_tXeGN"
   },
   "outputs": [],
   "source": [
    "# Remove questions and answers that are shorter than 2 words and longer than 20 words.\n",
    "min_line_length = 2\n",
    "max_line_length = 20\n",
    "\n",
    "# Filter out the questions that are too short/long\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
    "        short_answers.append(answer)\n",
    "        short_questions.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZjLY0EFXeGj"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for the frequency of the vocabulary\n",
    "vocab = {}\n",
    "for question in short_questions:\n",
    "    for word in question.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1\n",
    "            \n",
    "for answer in short_answers:\n",
    "    for word in answer.split():\n",
    "        if word not in vocab:\n",
    "            vocab[word] = 1\n",
    "        else:\n",
    "            vocab[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDEqThy4XeGt"
   },
   "outputs": [],
   "source": [
    "threshold = 7\n",
    "count = 0\n",
    "for k,v in vocab.items():\n",
    "    if v >= threshold:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJzGCrBjXeHH"
   },
   "outputs": [],
   "source": [
    "\n",
    "questions_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        questions_vocab_to_int[word] = word_num\n",
    "        word_num += 1\n",
    "        \n",
    "answers_vocab_to_int = {}\n",
    "\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        answers_vocab_to_int[word] = word_num\n",
    "        word_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "18xB6rWGXeHS"
   },
   "outputs": [],
   "source": [
    "# Add the unique tokens to the vocabulary dictionaries.\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "for code in codes:\n",
    "    questions_vocab_to_int[code] = len(questions_vocab_to_int)+1\n",
    "    \n",
    "for code in codes:\n",
    "    answers_vocab_to_int[code] = len(answers_vocab_to_int)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fc1nd3hEXeHb"
   },
   "outputs": [],
   "source": [
    "# Create dictionaries to map the unique integers to their respective words.\n",
    "# i.e. an inverse dictionary for vocab_to_int.\n",
    "questions_int_to_vocab = {v_i: v for v, v_i in questions_vocab_to_int.items()}\n",
    "answers_int_to_vocab = {v_i: v for v, v_i in answers_vocab_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rFHrs-lZXeH0"
   },
   "outputs": [],
   "source": [
    "# Add the end of sentence token to the end of every answer.\n",
    "for i in range(len(short_answers)):\n",
    "    short_answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHHhNIcCXeH9"
   },
   "outputs": [],
   "source": [
    "# Convert the text to integers. \n",
    "# Replace any words that are not in the respective vocabulary with <UNK> \n",
    "questions_int = []\n",
    "for question in short_questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questions_vocab_to_int:\n",
    "            ints.append(questions_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(questions_vocab_to_int[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in short_answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answers_vocab_to_int:\n",
    "            ints.append(answers_vocab_to_int['<UNK>'])\n",
    "        else:\n",
    "            ints.append(answers_vocab_to_int[word])\n",
    "    answers_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yE2AKOQXeIV"
   },
   "outputs": [],
   "source": [
    "# Calculate what percentage of all words have been replaced with <UNK>\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "for question in questions_int:\n",
    "    for word in question:\n",
    "        if word == questions_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1\n",
    "    \n",
    "for answer in answers_int:\n",
    "    for word in answer:\n",
    "        if word == answers_vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "        word_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fmUVS6MfXeIj"
   },
   "outputs": [],
   "source": [
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for length in range(1, max_line_length+1):\n",
    "    for i in enumerate(questions_int):\n",
    "        if len(i[1]) == length:\n",
    "            sorted_questions.append(questions_int[i[0]])\n",
    "            sorted_answers.append(answers_int[i[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PWKSDMN5jxKI"
   },
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    with tf.name_scope('model_inputs') as scope:\n",
    "        inputs = tf.placeholder(tf.int32, [None, None], name='input_sequence')\n",
    "        print(inputs.name)\n",
    "        targets = tf.placeholder(tf.int32, [None, None], name='target_sequence')\n",
    "        source_sequence_length = tf.placeholder(tf.int32,[None], name='source_sequence_length')\n",
    "        target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "        max_target_len = tf.reduce_max(target_sequence_length)    \n",
    "        return inputs, targets, source_sequence_length, target_sequence_length, max_target_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N6ciu-e_j2-i"
   },
   "outputs": [],
   "source": [
    "def hyperparam_inputs():\n",
    "    with tf.name_scope('hyperparam_inputs') as scope:\n",
    "        lr_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        return lr_rate, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guf3kZw5j7w0"
   },
   "outputs": [],
   "source": [
    "def process_decoder_input(target_data, target_vocab_to_int, batch_size):\n",
    "    with tf.name_scope('process_decoder_input') as scope:\n",
    "        after_slicing = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1],\n",
    "                                         name='strided_slice')\n",
    "        after_concat = tf.concat( [tf.fill([batch_size, 1], target_vocab_to_int['<GO>']), \n",
    "                                   after_slicing], 1,name='concatGo')\n",
    "        return after_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HeEsUBBkkE1o"
   },
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_vocab_size, \n",
    "                   encoding_embedding_size,\n",
    "                   isBiDirectional):\n",
    "    with tf.name_scope('encoding_layer') as scope:\n",
    "        embed = tf.contrib.layers.embed_sequence(rnn_inputs, \n",
    "                                                 vocab_size=source_vocab_size, \n",
    "                                                 embed_dim=encoding_embedding_size)\n",
    "        stacked_cells_fw = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "        \n",
    "        stacked_cells_bw = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.DropoutWrapper(\n",
    "            tf.contrib.rnn.LSTMCell(rnn_size), keep_prob) for _ in range(num_layers)])\n",
    "        \n",
    "        \n",
    "        if(False == isBiDirectional):\n",
    "          outputs, state = tf.nn.dynamic_rnn(stacked_cells_fw, \n",
    "                                             embed, \n",
    "                                             dtype=tf.float32)\n",
    "        else:\n",
    "          outputs, state = tf.nn.bidirectional_dynamic_rnn(cell_fw = stacked_cells_fw,\n",
    "                                                     cell_bw = stacked_cells_bw,\n",
    "                                                     sequence_length = None,\n",
    "                                                     inputs = embed, \n",
    "                                                     dtype=tf.float32)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d8t_ldiRkpdT"
   },
   "outputs": [],
   "source": [
    "def decoding_layer_train(initial_state, dec_cell, dec_embed_input, \n",
    "                         target_sequence_length, max_summary_length, \n",
    "                         output_layer, keep_prob):\n",
    "   with tf.name_scope('decoding_layer_train') as scope:\n",
    "        helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input, \n",
    "                                               target_sequence_length,name='TrainingHelper')\n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, \n",
    "                                                  initial_state, output_layer)\n",
    "\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_summary_length)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXOCi04rlDJX"
   },
   "outputs": [],
   "source": [
    "def decoding_layer_infer(initial_state, dec_cell, dec_embeddings, start_of_sequence_id,\n",
    "                         end_of_sequence_id, max_target_sequence_length,\n",
    "                         vocab_size, output_layer, batch_size, keep_prob):\n",
    "   with tf.name_scope('decoding_layer_infer') as scope:\n",
    "        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                      tf.fill([batch_size], start_of_sequence_id), \n",
    "                                                      end_of_sequence_id)   \n",
    "        decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, helper, \n",
    "                                                  initial_state, output_layer)\n",
    "        outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, \n",
    "                                                      impute_finished=True, \n",
    "                                                      maximum_iterations=max_target_sequence_length)\n",
    "       \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QkHmn_-lSso"
   },
   "outputs": [],
   "source": [
    "def decoding_layer(dec_input, enc_outputs, encoder_state,\n",
    "                   source_sequence_length, target_sequence_length, \n",
    "                   max_target_sequence_length,\n",
    "                   rnn_size, num_layers,\n",
    "                   target_vocab_to_int, target_vocab_size,\n",
    "                   batch_size, keep_prob, \n",
    "                   decoding_embedding_size,\n",
    "                   withAttentionLayer):\n",
    "    with tf.name_scope('decoding_layer') as scope:\n",
    "        dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size+1, \n",
    "                                                        decoding_embedding_size],\n",
    "                                                       name='dec_embeddings'))\n",
    "        dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input,name='dec_embed_input')\n",
    "        cells = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(rnn_size) \n",
    "                                             for _ in range(num_layers)])\n",
    "        \n",
    "        initial_state = encoder_state\n",
    "        if(True == withAttentionLayer):\n",
    "          attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                          enc_outputs,\n",
    "                                                          source_sequence_length,\n",
    "                                                          name = \"Bahdanau\")\n",
    "          cells = tf.contrib.seq2seq.AttentionWrapper(cells,\n",
    "                                                     attn_mech,\n",
    "                                                     rnn_size,\n",
    "                                                     alignment_history=False)\n",
    "          initial_state = cells.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        output_layer = tf.layers.Dense(target_vocab_size,name='output_layer')\n",
    "        train_output = decoding_layer_train(initial_state, \n",
    "                                            cells, \n",
    "                                            dec_embed_input, \n",
    "                                            target_sequence_length, \n",
    "                                            max_target_sequence_length, \n",
    "                                            output_layer, \n",
    "                                            keep_prob)\n",
    "    \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        infer_output = decoding_layer_infer(initial_state, \n",
    "                                            cells, \n",
    "                                            dec_embeddings, \n",
    "                                            target_vocab_to_int['<GO>'], \n",
    "                                            target_vocab_to_int['<EOS>'], \n",
    "                                            max_target_sequence_length, \n",
    "                                            target_vocab_size, \n",
    "                                            output_layer,\n",
    "                                            batch_size,\n",
    "                                            keep_prob)\n",
    "\n",
    "    return (train_output, infer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UidXqsup6F5D"
   },
   "outputs": [],
   "source": [
    "def mergeBiDirectionalOutput(enc_states, num_layers):\n",
    "  fw_state = enc_states[0]\n",
    "  bw_state = enc_states[1]\n",
    "  lstm_c = []\n",
    "  lstm_h = []\n",
    "  \n",
    "  for i in range(num_layers):\n",
    "    lstm_c.append(tf.concat((fw_state[i].c,bw_state[i].c),1))\n",
    "    lstm_h.append(tf.concat((fw_state[i].h,bw_state[i].h),1))\n",
    "  \n",
    "  listForLstmTupleObj = []\n",
    "  for i in range(num_layers):\n",
    "    listForLstmTupleObj.append(tf.contrib.rnn.LSTMStateTuple(c = lstm_c[i], h = lstm_h[i]))\n",
    " \n",
    "  tup = tuple(listForLstmTupleObj)\n",
    "  return tup\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9eitkOxqlio3"
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, \n",
    "                  keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length, \n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, \n",
    "                  target_vocab_to_int, \n",
    "                  isBiDirectional, withAttentionLayer):\n",
    "    with tf.name_scope('seq2seq_model') as scope:\n",
    "        enc_outputs, enc_states = encoding_layer(input_data, \n",
    "                                             rnn_size, \n",
    "                                             num_layers, \n",
    "                                             keep_prob, \n",
    "                                             source_vocab_size, \n",
    "                                             enc_embedding_size,\n",
    "                                             isBiDirectional)\n",
    "        cell_size = rnn_size\n",
    "        \n",
    "        if(True == isBiDirectional):\n",
    "          enc_states = mergeBiDirectionalOutput(enc_states, num_layers)\n",
    "          cell_size = rnn_size * 2\n",
    "          \n",
    "          #Concat output\n",
    "          fw_cell_op = enc_outputs[0]\n",
    "          bw_cell_op = enc_outputs[1]\n",
    "          enc_outputs = tf.concat((fw_cell_op,bw_cell_op),axis=2)\n",
    "          \n",
    "        \n",
    "        dec_input = process_decoder_input(target_data, \n",
    "                                      target_vocab_to_int, \n",
    "                                      batch_size)\n",
    "        \n",
    "        train_output, infer_output = decoding_layer(dec_input,\n",
    "                                                    enc_outputs,\n",
    "                                                    enc_states, \n",
    "                                                    source_sequence_length,\n",
    "                                                    target_sequence_length, \n",
    "                                                    max_target_sentence_length,\n",
    "                                                    cell_size,\n",
    "                                                    num_layers,\n",
    "                                                    target_vocab_to_int,\n",
    "                                                    target_vocab_size,\n",
    "                                                    batch_size,\n",
    "                                                    keep_prob,\n",
    "                                                    dec_embedding_size,\n",
    "                                                    withAttentionLayer)\n",
    "        \n",
    "    return train_output, infer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cA2Nt4IMXeKG"
   },
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 200\n",
    "num_layers = 1 \n",
    "rnn_size = 512\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.005\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "keep_probability = 0.75\n",
    "isBiDirectional = True\n",
    "withAttentionLayer = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tx2QHWcOl8T2",
    "outputId": "fd73ab04-c0a7-4469-ddd9-deea3afe2ab2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_graph.as_default/model_inputs/input_sequence:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "source_vocab_to_int = questions_vocab_to_int\n",
    "target_vocab_to_int = answers_vocab_to_int\n",
    "\n",
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "checkpoint = 'chatbot/model'\n",
    "\n",
    "with train_graph.as_default():\n",
    "    with tf.name_scope('train_graph.as_default') as scope:\n",
    "        sess = tf.InteractiveSession()\n",
    "        input_data, targets, source_sequence_length, target_sequence_length, max_target_sequence_length = model_inputs()\n",
    "        lr, keep_prob = hyperparam_inputs()\n",
    "        train_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                   targets,\n",
    "                                                   keep_prob,\n",
    "                                                   batch_size,\n",
    "                                                   source_sequence_length,\n",
    "                                                   target_sequence_length,\n",
    "                                                   max_target_sequence_length,\n",
    "                                                   len(source_vocab_to_int),\n",
    "                                                   len(target_vocab_to_int),\n",
    "                                                   encoding_embedding_size,\n",
    "                                                   decoding_embedding_size,\n",
    "                                                   rnn_size,\n",
    "                                                   num_layers,\n",
    "                                                   target_vocab_to_int,\n",
    "                                                   isBiDirectional,\n",
    "                                                   withAttentionLayer)\n",
    "        sess.close()\n",
    "        training_logits = tf.identity(train_logits.rnn_output, name='logits')\n",
    "        inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "        masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, \n",
    "                                 dtype=tf.float32, name='masks')\n",
    "\n",
    "        with tf.name_scope(\"optimization\"):\n",
    "            cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "            gradients = optimizer.compute_gradients(cost)\n",
    "            capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "            train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Am6YmO4eXeKa"
   },
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch, vocab_to_int):\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGbsMiHBXeKi"
   },
   "outputs": [],
   "source": [
    "def batch_data(questions, answers, batch_size):\n",
    "    \"\"\"Batch questions and answers together\"\"\"\n",
    "    for batch_i in range(0, len(questions)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        questions_batch = questions[start_i:start_i + batch_size]\n",
    "        answers_batch = answers[start_i:start_i + batch_size]\n",
    "        pad_questions_batch = np.array(pad_sentence_batch(questions_batch, questions_vocab_to_int))\n",
    "        pad_answers_batch = np.array(pad_sentence_batch(answers_batch, answers_vocab_to_int))\n",
    "        answer_batch_size = []\n",
    "        for target in pad_answers_batch:\n",
    "            answer_batch_size.append(len(target))\n",
    "        question_batch_size = []\n",
    "        for source in pad_questions_batch:\n",
    "            question_batch_size.append(len(source))        \n",
    "        yield pad_questions_batch, pad_answers_batch, question_batch_size, answer_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNVYs_zmXeKs"
   },
   "outputs": [],
   "source": [
    "# Validate the training with 15% of the data\n",
    "train_valid_split = int(len(sorted_questions)*0.15)\n",
    "\n",
    "# Split the questions and answers into training and validating data\n",
    "train_questions = sorted_questions[train_valid_split:]\n",
    "train_answers = sorted_answers[train_valid_split:]\n",
    "\n",
    "valid_questions = sorted_questions[:train_valid_split]\n",
    "valid_answers = sorted_answers[:train_valid_split]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 763
    },
    "colab_type": "code",
    "id": "zlj-3fwYm4Zy",
    "outputId": "b30f7611-00f6-4555-ff6b-29262f3dea15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 Batch    0/587 - Loss:  0.093, Seconds: 1650.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-1ac70d3f0c3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m                    \u001b[0msource_sequence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msources_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                    \u001b[0mtarget_sequence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                    keep_prob: keep_probability})\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m           \u001b[0mtotal_train_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorenviron\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "display_step = 100\n",
    "stop_early = 0 \n",
    "stop = 5\n",
    "validation_check = ((len(train_questions))//batch_size//2)-1 \n",
    "total_train_loss = 0\n",
    "summary_valid_loss = [] \n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(1, epochs+1):\n",
    "      for batch_i, (source_batch, target_batch, sources_lengths, targets_lengths) in enumerate(\n",
    "              batch_data(train_questions, train_answers, batch_size)):\n",
    "          start_time = time.time()\n",
    "          _, loss = sess.run(\n",
    "                  [train_op, cost],\n",
    "                  {input_data: source_batch,\n",
    "                   targets: target_batch,\n",
    "                   lr: learning_rate,\n",
    "                   source_sequence_length: sources_lengths,\n",
    "                   target_sequence_length: targets_lengths,\n",
    "                   keep_prob: keep_probability})\n",
    "\n",
    "          total_train_loss += loss\n",
    "          end_time = time.time()\n",
    "          batch_time = end_time - start_time\n",
    "\n",
    "          if batch_i % display_step == 0:\n",
    "              print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                    .format(epoch_i,\n",
    "                            epochs, \n",
    "                            batch_i, \n",
    "                            len(train_questions) // batch_size, \n",
    "                            total_train_loss / display_step, \n",
    "                            batch_time*display_step))\n",
    "              total_train_loss = 0\n",
    "\n",
    "          if batch_i % validation_check == 0 and batch_i > 0:\n",
    "              total_valid_loss = 0\n",
    "              start_time = time.time()\n",
    "              for batch_ii, (questions_batch, answers_batch, question_lengths, answer_lengths) in \\\n",
    "                      enumerate(batch_data(valid_questions, valid_answers, batch_size)):\n",
    "                  valid_loss = sess.run(cost, {input_data: questions_batch,\n",
    "                                                  targets: answers_batch,\n",
    "                                                       lr: learning_rate,\n",
    "                                   source_sequence_length: question_lengths,\n",
    "                                   target_sequence_length: answer_lengths,\n",
    "                                                keep_prob: 1})\n",
    "                  total_valid_loss += valid_loss\n",
    "              end_time = time.time()\n",
    "              batch_time = end_time - start_time\n",
    "              avg_valid_loss = total_valid_loss / (len(valid_questions) / batch_size)\n",
    "              print('Valid Loss: {:>6.3f}, Seconds: {:>5.2f}'.format(avg_valid_loss, batch_time))\n",
    "\n",
    "              # Reduce learning rate, but not below its minimum value\n",
    "              learning_rate *= learning_rate_decay\n",
    "              if learning_rate < min_learning_rate:\n",
    "                  learning_rate = min_learning_rate\n",
    "\n",
    "              summary_valid_loss.append(avg_valid_loss)\n",
    "              if avg_valid_loss <= min(summary_valid_loss):\n",
    "                  print('New Record!') \n",
    "                  stop_early = 0\n",
    "                  saver.save(sess, checkpoint)\n",
    "\n",
    "              else:\n",
    "                  print(\"No Improvement.\")\n",
    "                  stop_early += 1\n",
    "                  if stop_early == stop:\n",
    "                      break\n",
    "\n",
    "      if stop_early == stop:\n",
    "          print(\"Stopping Training.\")\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnzdHOJ90MF0"
   },
   "outputs": [],
   "source": [
    "def question_to_seq(question, vocab_to_int):\n",
    "    '''Prepare the question for the model'''\n",
    "    \n",
    "    question = clean_text(question)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "id": "HZWI7npXXeLL",
    "outputId": "a2623110-e797-4cb4-8b5e-867426c3e45f"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "metagraph = \"chatbot/model.meta\" \n",
    "random = np.random.choice(len(short_questions))\n",
    "input_question = short_questions[random]\n",
    "batch_size  = 200\n",
    "# Prepare the question\n",
    "input_question = question_to_seq(input_question, questions_vocab_to_int)\n",
    "\n",
    "# Pad the questions until it equals the max_line_length\n",
    "input_question = input_question + [questions_vocab_to_int[\"<PAD>\"]] * (max_line_length - len(input_question))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.import_meta_graph(metagraph)\n",
    "    saver.restore(sess,checkpoint)\n",
    "    graph = sess.graph\n",
    "    logit =  graph.get_tensor_by_name('train_graph.as_default/predictions:0')\n",
    "    input_source = graph.get_tensor_by_name('train_graph.as_default/model_inputs/input_sequence:0')\n",
    "    input_len = graph.get_tensor_by_name('train_graph.as_default/model_inputs/source_sequence_length:0')\n",
    "    output_len = graph.get_tensor_by_name('train_graph.as_default/model_inputs/target_sequence_length:0')\n",
    "    learning_rate = graph.get_tensor_by_name('train_graph.as_default/hyperparam_inputs/keep_prob:0')\n",
    "    \n",
    "    output = sess.run(logit, {input_source: [input_question]*batch_size,\n",
    "                                         input_len: [len(input_question)]*batch_size, \n",
    "                                         output_len: [len(input_question)*2]*batch_size,\n",
    "                                         learning_rate: 1.0})\n",
    "    translate_logits = output[0]\n",
    "    \n",
    "    pad_q = questions_vocab_to_int[\"<PAD>\"]\n",
    "    pad_a = answers_vocab_to_int[\"<PAD>\"]\n",
    "\n",
    "    print(\"\\n translate_logits:\",translate_logits.shape)\n",
    "    print('Input')\n",
    "    print('  Word Ids:      {}'.format([i for i in input_question]))\n",
    "    print('  English Words: {}'.format([questions_int_to_vocab[i] for i in input_question]))\n",
    "\n",
    "    print('\\nPrediction')\n",
    "    print('  Word Ids:      {}'.format([i for i in translate_logits]))\n",
    "    print('  French Words: {}'.format(\" \".join([answers_int_to_vocab[i] for i in translate_logits])))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chatbot_version2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
